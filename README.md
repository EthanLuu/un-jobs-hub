# ğŸŒ UNJobsHub

> **Your gateway to United Nations careers.** Find, filter, and apply to jobs across the UN system with AI-powered matching.

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Version](https://img.shields.io/badge/version-1.10.0-blue.svg)](https://github.com/yourusername/un-jobs-hub/releases)
[![Next.js](https://img.shields.io/badge/Next.js-15-black)](https://nextjs.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109-green)](https://fastapi.tiangolo.com/)
[![Python](https://img.shields.io/badge/Python-3.11+-blue)](https://www.python.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5-blue)](https://www.typescriptlang.org/)

## ğŸ“‹ Overview

UNJobsHub is a comprehensive job search platform that aggregates positions from across the United Nations system, including UN Secretariat, UNDP, UNICEF, WHO, FAO, UNOPS, and more. The platform features:

- ğŸ” **Smart Job Search** - Advanced filtering across 8 UN organizations with full-text search
- ğŸ¤– **AI-Powered Matching** - Multi-dimensional matching (TF-IDF, fuzzy matching, skill weighting)
- ğŸ“„ **Resume Analysis** - Upload and parse PDF/DOCX resumes with intelligent field extraction
- â­ **Favorites & Tracking** - Save jobs and track application status
- ğŸ”” **Job Alerts** - Custom notifications for new matching positions
- ğŸ•·ï¸ **Automated Crawling** - Daily updates from 8 UN organizations with health monitoring
- ğŸ“Š **Performance Monitoring** - Real-time API metrics and slow request detection
- ğŸ” **Enterprise Security** - Rate limiting, input validation, security headers
- ğŸ› ï¸ **Developer Tools** - Database migration tools, CLI utilities, structured logging

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend (Next.js 15 + Tailwind 4)   â”‚
â”‚  Vercel Deployment â”‚ App Router + SWR  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend API (FastAPI + PostgreSQL)     â”‚
â”‚  Railway Deployment â”‚ JWT Auth â”‚ Celery â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Crawlers (Playwright + Scrapy)         â”‚
â”‚  GitHub Actions / Railway Cron          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Node.js 18+
- Docker & Docker Compose
- PostgreSQL 16
- Redis 7

### Installation

1. **Clone the repository:**
```bash
git clone https://github.com/yourusername/un-jobs-hub.git
cd un-jobs-hub
```

2. **Set up environment variables:**
```bash
cp .env.example .env
# Edit .env with your configuration
```

3. **Install dependencies:**
```bash
make install
```

4. **Initialize database:**
```bash
# Check database connection
python backend/init_db.py check

# Create tables
python backend/init_db.py tables

# Seed test data (development only)
python backend/init_db.py seed
```

5. **Run development servers:**
```bash
# Terminal 1: Backend
make backend

# Terminal 2: Frontend
make frontend
```

Or run both simultaneously:
```bash
make dev
```

Visit:
- Frontend: http://localhost:3000
- Backend API: http://localhost:8000
- API Docs: http://localhost:8000/docs

## ğŸ“¦ Project Structure

```
un-jobs-hub/
â”œâ”€â”€ frontend/                 # Next.js 15 application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/             # App Router pages
â”‚   â”‚   â”‚   â”œâ”€â”€ [locale]/   # Internationalized routes
â”‚   â”‚   â”‚   â”œâ”€â”€ auth/       # Authentication pages
â”‚   â”‚   â”‚   â””â”€â”€ api/        # API routes
â”‚   â”‚   â”œâ”€â”€ components/      # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ ui/         # Shadcn/UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ home/       # Home page components
â”‚   â”‚   â”‚   â”œâ”€â”€ jobs/       # Job listing components
â”‚   â”‚   â”‚   â””â”€â”€ layout/     # Layout components
â”‚   â”‚   â””â”€â”€ lib/            # Utilities & API client
â”‚   â”‚       â”œâ”€â”€ api.ts      # Enhanced API client with retry
â”‚   â”‚       â”œâ”€â”€ api-errors.ts # Typed error handling
â”‚   â”‚       â””â”€â”€ lazy-load.tsx # Lazy loading utilities
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tailwind.config.ts
â”‚
â”œâ”€â”€ backend/                  # FastAPI application
â”‚   â”œâ”€â”€ main.py              # Application entry point
â”‚   â”œâ”€â”€ config.py            # Settings & configuration
â”‚   â”œâ”€â”€ database.py          # Database connection
â”‚   â”œâ”€â”€ init_db.py           # Database initialization CLI
â”‚   â”œâ”€â”€ migrate_db.py        # Migration helper CLI
â”‚   â”œâ”€â”€ models/              # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ user.py
â”‚   â”‚   â”œâ”€â”€ job.py
â”‚   â”‚   â”œâ”€â”€ favorite.py
â”‚   â”‚   â”œâ”€â”€ resume.py
â”‚   â”‚   â””â”€â”€ subscription.py
â”‚   â”œâ”€â”€ schemas/             # Pydantic schemas
â”‚   â”œâ”€â”€ routers/             # API endpoints
â”‚   â”‚   â”œâ”€â”€ auth.py
â”‚   â”‚   â”œâ”€â”€ jobs.py
â”‚   â”‚   â”œâ”€â”€ favorites.py
â”‚   â”‚   â”œâ”€â”€ resume.py
â”‚   â”‚   â”œâ”€â”€ match.py
â”‚   â”‚   â”œâ”€â”€ crawl.py
â”‚   â”‚   â””â”€â”€ metrics.py       # Monitoring endpoints
â”‚   â”œâ”€â”€ services/            # Business logic
â”‚   â”‚   â”œâ”€â”€ resume_parser.py
â”‚   â”‚   â””â”€â”€ matching_service.py  # Enhanced AI matching
â”‚   â”œâ”€â”€ crawlers/            # Web scrapers (8 organizations)
â”‚   â”‚   â”œâ”€â”€ base_crawler.py   # Enhanced base with monitoring
â”‚   â”‚   â”œâ”€â”€ un_careers_spider.py
â”‚   â”‚   â”œâ”€â”€ uncareer_spider.py
â”‚   â”‚   â”œâ”€â”€ who_spider.py
â”‚   â”‚   â”œâ”€â”€ fao_spider.py
â”‚   â”‚   â”œâ”€â”€ unops_spider.py
â”‚   â”‚   â”œâ”€â”€ ilo_spider.py
â”‚   â”‚   â”œâ”€â”€ undp_spider.py
â”‚   â”‚   â””â”€â”€ unicef_spider.py
â”‚   â”œâ”€â”€ utils/               # Helper functions
â”‚   â”‚   â”œâ”€â”€ auth.py
â”‚   â”‚   â”œâ”€â”€ logger.py         # Structured logging
â”‚   â”‚   â”œâ”€â”€ monitoring.py     # Performance monitoring
â”‚   â”‚   â”œâ”€â”€ crawler_monitoring.py  # Crawler health checks
â”‚   â”‚   â”œâ”€â”€ keyword_extraction.py  # TF-IDF extraction
â”‚   â”‚   â”œâ”€â”€ cache.py          # Redis cache manager
â”‚   â”‚   â”œâ”€â”€ rate_limit.py     # API rate limiting
â”‚   â”‚   â””â”€â”€ security.py       # Security utilities
â”‚   â”œâ”€â”€ alembic/             # Database migrations
â”‚   â”œâ”€â”€ celery_app.py        # Celery configuration
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/           # CI/CD pipelines
â”‚       â””â”€â”€ ci-cd.yml        # Automated testing & deployment
â”œâ”€â”€ docker-compose.yml        # Local development
â”œâ”€â”€ Makefile                  # Development commands
â”œâ”€â”€ PROJECT_STATUS.md         # Project status report
â”œâ”€â”€ CHANGELOG.md              # Detailed version history
â””â”€â”€ README.md
```

## ğŸ”§ Technology Stack

### Frontend
- **Framework:** Next.js 15 (App Router, React 19)
- **Styling:** Tailwind CSS 4.0
- **UI Components:** Shadcn/UI + Radix UI
- **State Management:** SWR for data fetching
- **Animations:** Framer Motion
- **Type Safety:** TypeScript 5

### Backend
- **Framework:** FastAPI 0.109
- **Database:** PostgreSQL 16 (via SQLAlchemy 2.0)
- **Cache/Queue:** Redis 7
- **Task Queue:** Celery 5.3
- **Authentication:** JWT (python-jose)
- **Web Scraping:** Playwright + Scrapy
- **AI/ML:** OpenAI API, Sentence Transformers

### DevOps & Deployment
- **Frontend Hosting:** Vercel
- **Backend Hosting:** Railway
- **Database:** Supabase / Railway PostgreSQL
- **Cache:** Upstash Redis
- **CI/CD:** GitHub Actions
- **Containerization:** Docker & Docker Compose

## ğŸ“¡ API Endpoints

### Authentication
- `POST /api/auth/register` - Register new user
- `POST /api/auth/login` - User login
- `GET /api/auth/me` - Get current user

### Jobs
- `GET /api/jobs` - List jobs (with filters & pagination)
- `GET /api/jobs/{id}` - Get job details
- `GET /api/jobs/filters/options` - Get filter options

### Favorites
- `GET /api/favorites` - List user's favorites
- `POST /api/favorites` - Add job to favorites
- `PUT /api/favorites/{id}` - Update favorite
- `DELETE /api/favorites/{id}` - Remove favorite

### Resume
- `POST /api/resume/upload` - Upload resume
- `GET /api/resume` - List user's resumes
- `GET /api/resume/{id}` - Get resume details
- `DELETE /api/resume/{id}` - Delete resume

### Matching
- `POST /api/match` - Match resume to jobs
- `GET /api/match/job/{job_id}` - Match to specific job

### Crawler (Admin Only)
- `POST /api/crawl/trigger` - Trigger crawler
- `GET /api/crawl/status/{task_id}` - Get crawler status
- `POST /api/crawl/trigger-all` - Trigger all crawlers
- `GET /api/crawl/health` - Get overall crawler health
- `GET /api/crawl/health/{organization}` - Get specific crawler health
- `GET /api/crawl/stats` - Get aggregated crawler statistics

### Monitoring & Metrics
- `GET /api/health` - Health check endpoint (database connection status)
- `GET /api/metrics` - System metrics (job counts, user stats, org distribution)

## ğŸ¤– AI Features

### Resume Parsing
The system automatically extracts:
- Skills and competencies
- Years of experience
- Education history
- Key achievements

### Job Matching Algorithm

**Multi-dimensional scoring system:**
```python
final_score = (
    keyword_match * 0.40 +      # TF-IDF + fuzzy matching + importance weighting
    experience_match * 0.25 +    # Graduated scoring with years of experience
    education_match * 0.15 +     # Education level comparison
    language_match * 0.12 +      # Language proficiency (fuzzy matching)
    location_match * 0.08        # Geographic preferences
)
```

**Advanced Features:**
- **TF-IDF Keyword Extraction** - Intelligent phrase detection with N-grams
- **Fuzzy Matching** - Levenshtein distance algorithm (0.8 threshold)
- **Skill Importance Weighting** - Technical skills weighted 1.4-1.5x
- **Graduated Scoring** - Progressive penalties instead of hard cutoffs
- **Redis Caching** - 1-hour cache for match results

Factors considered:
- Keyword overlap with importance weighting (40%)
- Required vs. actual years of experience (25%)
- Education level requirements (15%)
- Language proficiency with fuzzy matching (12%)
- Geographic location preferences (8%)

### OpenAI Integration
When configured, the system uses GPT-4 to provide:
- Personalized application recommendations
- Gap analysis between candidate and requirements
- Career development suggestions

## ğŸ•·ï¸ Web Crawlers

### Supported Organizations (8 Total)
1. **UN Careers** - careers.un.org (Official UN Secretariat)
2. **UNCareer.net** - UNCareer.net (Internships, formal, vacant positions)
3. **WHO** - World Health Organization
4. **FAO** - Food and Agriculture Organization
5. **UNOPS** - United Nations Office for Project Services
6. **ILO** - International Labour Organization
7. **UNDP** - United Nations Development Programme
8. **UNICEF** - United Nations Children's Fund

### Crawler Features
- **Intelligent Field Extraction** - Responsibilities, requirements, education, experience
- **Multi-language Support** - Automatic language detection
- **Anti-bot Countermeasures** - User agents, delays, retry mechanisms
- **Error Handling** - Exponential backoff retry with detailed logging
- **Batch Processing** - Efficient database commits (every 10 records)
- **Health Monitoring** - Real-time health checks and metrics collection

### Crawler Schedule
- **Daily:** Staggered schedule (1-8 AM UTC) via Celery Beat
- **Manual:** Via admin dashboard or CLI
- **Rate Limiting:** Respects robots.txt and includes delays
- **Health Checks:** Three-level system (HEALTHY/WARNING/CRITICAL)

### Monitoring API
```bash
# Get overall crawler health
GET /api/crawl/health

# Get specific crawler details
GET /api/crawl/health/{organization}

# Get aggregated statistics
GET /api/crawl/stats
```

## ğŸš€ Deployment

### Frontend (Vercel)

1. Connect GitHub repository to Vercel
2. Set environment variables:
   - `NEXT_PUBLIC_API_URL`
3. Deploy automatically on push to `main`

```bash
# Or deploy manually
cd frontend
vercel --prod
```

### Backend (Railway)

1. Create new Railway project
2. Connect GitHub repository
3. Set environment variables (see `.env.example`)
4. Deploy:

```bash
railway up
```

### Database Setup

**Option 1: Supabase**
```bash
# Install Supabase CLI
npm install -g supabase

# Initialize
supabase init
supabase db push
```

**Option 2: Railway PostgreSQL**
- Add PostgreSQL plugin in Railway dashboard
- Copy DATABASE_URL to environment variables

## ğŸ“Š Development Commands

### Quick Start
```bash
# Install all dependencies
make install

# Run development servers
make dev

# Run backend only
make backend

# Run frontend only
make frontend
```

### Database Management
```bash
# Start databases (PostgreSQL + Redis)
make db-up

# Stop databases
make db-down

# Check database connection
python backend/init_db.py check

# Create all tables
python backend/init_db.py tables

# Seed test data (creates admin and test users)
python backend/init_db.py seed

# Show all tables
python backend/init_db.py show

# Drop all tables (requires confirmation)
python backend/init_db.py drop
```

### Database Migrations (Alembic)
```bash
# Create a new migration
python backend/migrate_db.py create "Add new field"

# Apply migrations
python backend/migrate_db.py upgrade

# Rollback migration
python backend/migrate_db.py downgrade

# Show current version
python backend/migrate_db.py current

# Show migration history
python backend/migrate_db.py history

# Stamp database version
python backend/migrate_db.py stamp <revision>
```

### Code Quality
```bash
# Run linters
make lint

# Format code
make format

# Type checking
make typecheck
```

### Build & Deploy
```bash
# Build production images
make build

# Start production stack
make up

# Stop production stack
make down

# Clean temp files
make clean
```

### Crawler Management
```bash
# Run all crawlers
python backend/run_all_crawlers.py

# Run specific crawler
python backend/crawlers/un_careers_spider.py

# Check crawler health
curl http://localhost:8000/api/crawl/health
```

## ğŸ§ª Testing

```bash
# Backend tests
cd backend
pytest

# Frontend tests
cd frontend
npm run test

# E2E tests
npm run test:e2e
```

## ğŸ” Security

### Authentication & Authorization
- **JWT-based authentication** with secure token generation
- **Password hashing** with bcrypt (strong hashing algorithm)
- **Role-based access control** (admin and user roles)

### Data Protection
- **SQL injection protection** via SQLAlchemy ORM
- **XSS protection** via React and Content Security Policy
- **Input validation** with length and format checks
- **File upload validation** with type and size restrictions

### Security Headers (v1.4.0)
- **X-Frame-Options** - Prevents clickjacking attacks
- **X-Content-Type-Options** - Prevents MIME sniffing
- **X-XSS-Protection** - Browser XSS protection
- **Strict-Transport-Security** - HSTS enforcement
- **Content-Security-Policy** - CSP protection
- **Referrer-Policy** - Referrer information control
- **Permissions-Policy** - Feature permissions

### API Security
- **Rate limiting** - Sliding window algorithm (v1.3.0)
  - Configurable limits per endpoint
  - IP and user-based throttling
  - Automatic 429 responses
- **CORS configuration** - Controlled cross-origin requests
- **Input sanitization** - Automatic input cleaning
- **Request timeouts** - 30-second default timeout (v1.8.0)

### Monitoring & Logging
- **Request ID tracking** - Distributed tracing support (v1.1.0)
- **Structured logging** - JSON format in production (v1.5.0)
- **Performance monitoring** - Slow request detection (v1.1.0)
- **Error tracking** - Comprehensive exception handling (v1.2.0)

## ğŸ“ Environment Variables

See `.env.example` for all required variables:

**Required:**
- `DATABASE_URL` - PostgreSQL connection string
- `SECRET_KEY` - JWT secret key
- `REDIS_URL` - Redis connection string

**Optional:**
- `OPENAI_API_KEY` - For AI features
- `SMTP_*` - For email notifications

## ğŸ“š Documentation

Comprehensive documentation is available in the following files:

- **[PROJECT_STATUS.md](PROJECT_STATUS.md)** - Complete project status, features, and roadmap
- **[CHANGELOG.md](CHANGELOG.md)** - Detailed version history from v1.0.0 to v1.10.0
- **[Backend README](backend/README.md)** - Backend architecture and API documentation
- **[Frontend README](frontend/README.md)** - Frontend architecture and component guide
- **[Deployment Guide](DEPLOYMENT.md)** - Production deployment instructions

### Version History Highlights

- **v1.10.0** - Comprehensive documentation (README, Backend, Frontend, Deployment guides)
- **v1.9.0** - Database migration and initialization tools
- **v1.8.0** - Frontend error handling with retry and toast notifications
- **v1.7.0** - Enhanced AI matching with TF-IDF and fuzzy matching
- **v1.6.0** - Crawler monitoring and health checks
- **v1.5.0** - Structured logging with JSON formatting
- **v1.4.0** - Security enhancements and database optimization
- **v1.3.0** - API rate limiting and Redis caching
- **v1.2.0** - Error handling and logging system
- **v1.1.0** - Performance monitoring and ILO crawler

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- UN system organizations for providing public job data
- Shadcn for the beautiful UI components
- FastAPI and Next.js communities

## ğŸ“§ Contact

For questions or support, please open an issue or contact [your-email@example.com](mailto:your-email@example.com).

---

**Built with â¤ï¸ for the UN community**



